# ImplementGAN
December 2016
### Summary
In this project, I reimplemented Generative Adversarial Networks on 3 datasets: 
1. A simple 1-d Gaussian distribution
2. MNIST dataset (vectorized)
3. MNIST dataset (images) - (implemented DCGAN)

### Details
Generative adversarial networks are fascinating models with their ability of mimicking a data distribution (and hence generating new data) without the need of complex structures like parametric models or Markov chains. Their working principle is often explained as a forgery-police game. They consist of two networks, generator and discriminator networks. Discriminator acts as the police that tries to identify if a bill is fake, i.e. generated by the forger, or real. Generator is the forger that generates fake bills and tries to trick the police. At each step, forger puts its fake bills into a stack with real bills, and the police tries to identify fake and real bills correctly.

In a more formal setting, both generator (G) and discriminator (D) are multi-layer neural networks. G is fed with random samples from a prior noise distribution pn(z), and then either the output of G or samples from an unknown data distribution pd(x) is fed to D. Both networks are simultaneously trained so that D will maximize the probability of assigning correct labels (e.g. 1 for real data, 0 for coming from G) to its input, and G will minimize the probability of D identifying G's output to D correctly.

GANs have their own challenges such as keeping generative and discriminative network in balance during training, and hardship of quantitatively measuring their success to assess their performance. However, they generate more real-like images compared to other models.

In this project, first I applied GANs to a Gaussian distribution. It was hard to find right parameters and even then, results were not that plausible, especially when the real data had large standard deviation since generator network was producing ”narrow” distributions compared to the real data. This is a known problem of GANs and attempts to fix this problem, as well as some others, is explained in the work of Salimans et al. [3].

<p align="center">
  <img src="/Figures/dist_NR10_M-2_STD0_5_DLR0_01_GLR0_05_2.png" width="350" title="Real Gaussian dist N(-2, 0.5) and generated dist from U(-10,10)">
  <img src="/Figures/dist_NR10_M3_STD0_2_DLR0_005_GLR0_005_s.png" width="350" title="Real Gaussian dist N(3, 0.2) and generated dist from U(-10,10)">
</p>

Then, I used MNIST images as 729x1 vectors as the input to D. When I apply GANs on MNIST data, I obtained more plausible results. Almost all of the generated figures were obvious to identify as the intended digit. Due to computational power restrictions, I could not train the networks for a very long time. Hence, there is a chance of improvement by training a little bit longer. Also, I was not able to fine-tune the parameters due to the lack of a numerical, quantified comparison of results. As future work, increasing training epochs and fine-tuning would give better results.

<p align="center">
  <img src="/Figures/image_L2_DLR0_100000_GLR0_100000_BS50_N100_gen2_disc4.png" width="300" title="Images for digit 2 generated by the 4-layer generative network trained on vector MNIST dataset">
</p>

Finally, I used MNIST images as 27x27 matrices as the input to a Deep Convolutional GAN. I could only train the network for 2 epochs due to computational power restrictions. The results were worse compared to vectorized images, and it was hard to identify the digits generated by DC-GAN. However, comparing these two results is not fair since the advanced DC-GAN network have not been fully trained at all.

<p align="center">
  <img src="/Figures/dcgan1.png" width="250" title="Random examples of generated digits by DC-GAN, trained for 1 epoch">
  <img src="/Figures/dcgan2.png" width="250" title="Random examples of generated digits by DC-GAN, trained for 2 epochs">
</p>

More details about the project can be found [here](tabak2_project_report.pdf).

### Code References
Some of the code in this repo is obtained from the below sources and modified accordingly:
1. https://github.com/AYLIEN/gan-intro
2. https://github.com/carpedm20/DCGAN-tensorflow

### References
[1] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio, “Generative adversarial nets,” in Advances in Neural Information Processing Systems, 2014, pp. 2672–2680.

[2] Alec Radford, Luke Metz, and Soumith Chintala, “Unsupervised representation learning with deep convolutional generative adversarial networks,” CoRR, vol. abs/1511.06434, 2015.

[3] Tim Salimans, Ian J. Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen, “Improved techniques for training gans,” CoRR, vol. abs/1606.03498, 2016.

[4] Yann LeCun, Corinna Cortes, and Christopher JC Burges, “The MNIST database of handwritten digits,” 1998.
